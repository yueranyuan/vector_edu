\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage[backend=biber]{biblatex}
\bibliography{paper}

\title{Deep learning on psychological arousal classification}


\author{
Yueran Yuan, Jeffrey Zhang, David Rayson, Kai-min Chang\\
Language Technologies Institute\\
Carnegie Mellon University\\
Pittsburgh, PA 15213\\
\texttt{\{yueranyuan, jzhang94, davidr92, kaimin.chang\}@gmail.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version, comment for submission version!

\begin{document}

\maketitle

\begin{abstract}
We present a task for psychological arousal state classification from EEG data and a model for classifying EEG data using deep learning and ensemble techniques. The task is classification of 10-second recordings taken from Emotiv EPOC headsets into the activities performed during each recording. We find that our model outperforms baseline accuracy values established by random forests and SVMs.
\end{abstract}

\section{Introduction} \label{section:introduction}
One of the ongoing challenges of brain-computer interface (BCI) research is the creation of a non-invasive device that can be directly and accurately controlled with one's mental processes. The most studied approach is electroencephalography (EEG), which is a technique that attaches several electrodes to the scalp to measure the electric potential changes caused by neural oscillations in the brain. EEG devices have been difficult to access for non-researchers until recently, as more EEG headsets become easier to use and available at a lower cost. However, the ability to reliably infer information from the brain using EEG such as intent is limited by poor spatial resolution and susceptibility to noise.

In the last decade, deep learning techniques have made significant advances in machine vision, natural language processing, and speech processing. This success is attributed to the ability of deep neural networks (DNNs) to learn complex and robust distributed representations of inputs \cite{DBLP:journals/corr/abs-1206-5538} when given large numbers of training examples. Because EEG data can be more easily acquired due to the availability of consumer-grade EEG headsets, DNNs present an opportunity for improving performance in BCI. Thus, we developed a deep learning model to classify a constrained set of EEG data.

In section \ref{section:litreview}, we present a detailed discussion of EEG and deep learning. In section \ref{section:data}, we discuss the characteristics of our EEG dataset, features used in EEG signal processing, and the task at hand. In section \ref{section:techniques}, we describe the deep learning and ensemble approaches used in our exploration of the problem. In section \ref{section:experiment}, we describe our experimental setup and evaluation metrics for the task. In section \ref{section:results}, we describe the results we obtained.


\section{Prior work} \label{section:litreview}
%TODO: so like our thing has never actually been done before. actually, this guy did this one thing but it really sucks. so we were like obviously we could do better with technology x.
%-EEG
% -what is EEG
% -EEG patterns (psych stuff/neuro stuff)
% -Dataset (brief)
%-Deep learning
% -what is deep learning
% -deep learning + EEG

\section{Data} \label{section:data}
%TODO: we have big data, oooh. okay maybe not that big but it still takes forever to load 
%-Dataset (details)
% -processing
%-Features
% -raw
% -dtft
%  -windows
% -wavelets
% -other

\section{Techniques} \label{section:techniques}
%TODO: we thought technology x would be a great idea because it has these advantages over the sucky way that one guy was doing it. so we wrote some code and did it. we wrote the code. :o
%-Deep learning
% -deep nets
% -dropout
% -autoencoder
% -denoising autoencoder
% -convolutional neural nets
% -parameterized rectifiers
Parameterized rectified linear units (PReLUs) \cite{DBLP:journals/corr/HeZR015} are leaky ReLUs whose slopes are learnable parameters.
% -batchnorm (plus all that other stuff)
Batch normalization \cite{DBLP:journals/corr/IoffeS15} is a method for training deep neural networks that makes normalization a part of the network, allowing for better performance and training speed.
%-Ensemble
% -random forest
% -classifier fusion

\section{Experiment} \label{section:experiment}
%TODO: we're not making up numbers here yo.
%-Cross-validation
%-Parameter search
%-Evaluation
% -baseline classifiers
% -auc, acc, best-cutoff
\subsection{Evaluation Metrics}
\subsubsection{Accuracy}
The output of our classifiers when classifying input $x$ into category $A$ or $B$ is two probabilities $P(A_x)$ and $P(B_x)$ (the predicted probability that $x$ is in category $A$ or $B$ respectively), with $P(A_x)+P(B_x)=1$.  We choose the greater of these probabilities and declare its category to be the predicted category for $x$.  The accuracy of the classifier over a set of input samples is then simply the percentage of samples for which the predicted and actual categories agree.
\subsubsection{Best-Cutoff Accuracy}
Best-cutoff accuracy is a generalization of our accuracy calculation described above.  The only change is in how the predicted category for a sample is determined: a cutoff $c$ is set, with the sample declared to be in category $A$ if $P(A_x) > c$ and category $B$ otherwise.  Each time that the best-cutoff accuracy is computed for a set of predictions, the value of the cutoff is dynamically chosen such that the final accuracy result is maximized.
\subsubsection{AUC}
The area under the receiver operating characteristic (ROC) curve has been proposed \cite{DBLP:conf/ijcai/LingHZ03} as a more effective metric for comparing classifiers than accuracy.  The ROC curve represents the ratio of the true positive rate of a classifier to its false positive rate as its discrimination cutoff is varied.  We arbitrarily select one class here to be the positive class and the other to be the negative class.  We then construct a ranking of all input samples from least-likely to most-likely to be in the positive class (as determined by the probability assigned to that class by the classifier).  Our discrimination cutoff is then the point in this ranking list for which we determine less-likely-positive samples to be negative and more-likely ones to be positive.  We calculate the area under the ROC curve over all possible values of this cutoff, using this result as our metric.
%-Alternative evaluations
%-Ranking channels for importance

\section{Results} \label{section:results}
%TODO: yeah so um our thing sucks like the other guy's stuff. but it sucks less. so yaya
%-the results

\section{Conclusion} \label{section:conclusion}
%TODO: look at what we did. we think we can do better next time. but yeah we'll need money for ramen so gib pls
%-refer back to the prev work
%-future work

\section*{Acknowledgments}
%TODO: thanks mom


\printbibliography

\end{document}
