\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage[backend=biber]{biblatex}
\bibliography{paper}

\title{Deep learning on psychological arousal classification}


\author{
Yueran Yuan, Jeffrey Zhang, David Rayson, Kai-min Chang\\
Language Technologies Institute\\
Carnegie Mellon University\\
Pittsburgh, PA 15213\\
\texttt{\{yueranyuan, jzhang94, davidr92, kaimin.chang\}@gmail.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version, comment for submission version!

\begin{document}

\maketitle

\begin{abstract}
We present a task for psychological arousal state classification from EEG data and a model for classifying EEG data using deep learning and ensemble techniques. The task is classification of 10-second recordings taken from Emotiv EPOC headsets into the activities performed during each recording. We find that our model outperforms baseline accuracy values established by random forests and SVMs.
\end{abstract}

\section{Introduction} \label{section:introduction}
One of the ongoing challenges of brain-computer interface (BCI) research is the creation of a non-invasive device that can be directly and accurately controlled with one's mental processes. The most studied approach is electroencephalography (EEG), which is a technique that attaches several electrodes to the scalp to measure the electric potential changes caused by neural oscillations in the brain. EEG devices have been difficult to access for non-researchers until recently, as more EEG headsets become easier to use and available at a lower cost. However, the ability to reliably infer information from the brain using EEG such as intent is limited by poor spatial resolution and susceptibility to noise.

In the last decade, deep learning techniques have made significant advances in machine vision, natural language processing, and speech processing. This success is attributed to the ability of deep neural networks (DNNs) to learn complex and robust distributed representations of inputs \cite{DBLP:journals/corr/abs-1206-5538} when given large numbers of training examples. Because EEG data can be more easily acquired due to the availability of consumer-grade EEG headsets, DNNs present an opportunity for improving performance in BCI. Thus, we developed a deep learning model to classify a constrained set of EEG data.

In section \ref{section:litreview}, we present a detailed discussion of EEG and deep learning. In section \ref{section:data}, we discuss the characteristics of our EEG dataset, features used in EEG signal processing, and the task at hand. In section \ref{section:techniques}, we describe the deep learning and ensemble approaches used in our exploration of the problem. In section \ref{section:experiment}, we describe our experimental setup and evaluation metrics for the task. In section \ref{section:results}, we describe the results we obtained.


\section{Prior work} \label{section:litreview}
\subsection{EEG}
The EEG (electroencephalogram) signal is a voltage signal that can be measured on the surface of the scalp, arising from large areas of coordinated neural activity. Rhythmic fluctuations (oscillation patterns) in the EEG signal occur within several particular frequency bands, and the relative level of activity within each frequency band has been associated with brain/emotional states such as focused attentional processing, engagement, and frustration (Marosi et al., 2002; Marosi et al., 2001;
Mundy-Castle, 1951; Berka et al., 2007) and cognitive and memory performance (Gruber et al., 2004; Fernandez et al., 1998; Gevins et al., 1997; Jensen et al., 2007; Klimesch, 1999). The discriminating power of EEG has been demonstrated in a number of studies, achieving 86\% accuracy on 14-way task discrimination, 95\% accuracy on discriminating high/low memory load (Gevins et al., 1998). 
\subsection{IAPS}
\subsection{Deep Learning}
Contemporary deep learning was introduced relatively recently (Hinton, 2006). Prior to the advent of deep learning, classification algorithms by in large operated on some pre-determined preprocessing steps which transforms the raw input into features. These features were then used by a learning algorithm to train a classifier. Deep algorithms, by contrast, combine data processing and classification in the same learning pipeline. This allows the system to learn how to process raw data from
patterns in the data rather than relying on hand-engineered pre-processing steps. This is why deep learning has alternatively been called feature learning or representation learning.
Deep learning excels when it is trained on large repositories of data. So the relatively recent availability of large datasets for speech, computer vision, and text processing has allowed deep learning researchers to train models matching or beating state-of-the-art systems (Boulanger-Lewandowski et al., 2012; Krizhevsky et al., 2012; Miklov et al., 2011).  Deep learning is particularly good at extracting structure and information from raw data with little to no knowledge about the
nature of the data or the nature of the intended classification task by performing unsupervised pre-training (Mesnil et al., 2012). Researchers hypothesize that this is because deep learning algorithms are able to disentangle the many sources of variation (Bengio, 2009).  This is supported by empirical analysis of the learnt features which demonstrate that features that best distinguish task domains are largely separate from features that best distinguish classification conditions (Glorot et
al., 2011), a separation that suggests features are selectively sensitive to only one type of variation.  The usefulness of this disentanglement is evident in the strong performance of deep learning algorithms in transfer learning competitions (Mesnil et al .,2012; Goodfellow et al., 2011).
Disentangling sources of variation is a particularly useful trait for use in EEG. As mentioned earlier, sources of signal in EEG are not well-understood and feature engineering is very difficult. By providing deep learning algorithms with a sufficiently large EEG dataset, the algorithms may be able to disentangle the many sources of noise and signal to recognize the specific patterns associated with mental states of interest and discarding patterns associated with environmental noise.
Further, deep algorithms excel at transferring between different task domains which may be helpful in creating classifiers that can be used on unseen subjects despite non-trivial differences in subject psychology and physiology.
\subsection{Deep Learning and EEG}
Deep learning has also had some very recent success in non-EEG biometrics and laboratory EEG. Mirowski et al. (2008) used a convolutional neural network on intracranial EEG signal, predicting seizures with zero-false-alarm rate on 20 out of 21 patients, beating SVM and logistic regression. Cecotti and Gr√§ser (2010) achieved state-of-the-art results on detecting the P300 event-related-potential with laboratory grade EEG also using convolutional networks. And more recently, Martinez et al.
(2013) used deep learning techniques to produce models of affect from non-EEG biometrics (Skin Conductance and Blood Volume Pulse) which perform significantly better than ad-hoc feature models.
Uniquely, we are using Deep Learning techniques on non-medical-grade EEG devices to track emotion.

\section{Data} \label{section:data}
Our experiment uses data from Siegle et al. (forthcoming). To our knowledge, this is the largest dataset of EEG collected in controlled laboratory conditions (TODO: get x and n) with n unique subjects from x different sites. The dataset contains various stimuli types and our experiments focus on the IAPS stimuli (TODO: cite lang et al 2008). IAPS is a set of images designed to illicit emotional responses along two emotional axes - high, low arousal and positive, negative valence. The stimuli
fall into 4 categories - low arousal positive valence, low arousal negative valence, high arousal positive valence, and high arousal negative valence. In this dataset, each subject viewed 10 images from each category with a 2 (TODO: check this) second fixation period between each stimulus category. One limitation of the experiment was that the images were shown to the subject in the same sequence.

The dataset was collected with Emotiv EPOC EEG devices. The EEG data has 14 channels at 128Hz.  The data was processed with %TODO: asdfasdfasdf how did siegle process the data??

We post-processed the resulting 128Hz signal with DFT and wavelets. We used 5 channel DFT threshholding at 0.1Hz, 4Hz, 8Hz, 12Hz, 30Hz, and 64Hz. We used Debauchey wavelets, trying a variety of wavelet decomposition depths. We also derived a number of features based on the feature set used in recent work of (citation needed) which includes eigen correlation, statistical moments, and Hjorth parameters.
%TODO: we have big data, oooh. okay maybe not that big but it still takes forever to load 
%-Dataset (details)
% -processing
%-Features
% -raw
% -dtft
%  -windows
% -wavelets
% -other

\section{Techniques} \label{section:techniques}
%TODO: we thought technology x would be a great idea because it has these advantages over the sucky way that one guy was doing it. so we wrote some code and did it. we wrote the code. :o
%-Deep learning
% -deep nets
% -dropout
% -autoencoder
% -denoising autoencoder
% -convolutional neural nets
% -parameterized rectifiers
Parameterized rectified linear units (PReLUs) \cite{DBLP:journals/corr/HeZR015} are leaky ReLUs whose slopes are learnable parameters.
% -batchnorm (plus all that other stuff)
Batch normalization \cite{DBLP:journals/corr/IoffeS15} is a method for training deep neural networks that makes normalization a part of the network, allowing for better performance and training speed.
%-Ensemble
% -random forest
% -classifier fusion

\section{Experiment} \label{section:experiment}
%TODO: we're not making up numbers here yo.
%-Cross-validation
%-Parameter search
%-Evaluation
% -baseline classifiers
% -auc, acc, best-cutoff
%-Alternative evaluations
%-Ranking channels for importance

\section{Results} \label{section:results}
%TODO: yeah so um our thing sucks like the other guy's stuff. but it sucks less. so yaya
%-the results

\section{Conclusion} \label{section:conclusion}
%TODO: look at what we did. we think we can do better next time. but yeah we'll need money for ramen so gib pls
%-refer back to the prev work
%-future work

\section*{Acknowledgments}
%TODO: thanks mom


\printbibliography

\end{document}
