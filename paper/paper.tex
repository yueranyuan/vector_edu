\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage[backend=biber]{biblatex}
\bibliography{paper}

\title{Deep learning on psychological arousal classification}


\author{
Yueran Yuan, Jeffrey Zhang, David Rayson, Kai-min Chang\\
Language Technologies Institute\\
Carnegie Mellon University\\
Pittsburgh, PA 15213\\
\texttt{\{yueranyuan, jzhang94, davidr92, kaimin.chang\}@gmail.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version, comment for submission version!

\begin{document}

\maketitle

\begin{abstract}
We present a task for psychological arousal state classification from EEG data and a model for classifying EEG data using deep learning and ensemble techniques. The task is classification of 10-second recordings taken from Emotiv EPOC headsets into the activities performed during each recording. We find that our model outperforms baseline accuracy values established by random forests and SVMs.
\end{abstract}

\section{Introduction} \label{section:introduction}
One of the ongoing challenges of brain-computer interface (BCI) research is the creation of a non-invasive device that can be directly and accurately controlled with one's mental processes. The most studied approach is electroencephalography (EEG), which is a technique that attaches several electrodes to the scalp to measure the electric potential changes caused by neural oscillations in the brain. EEG devices have been difficult to access for non-researchers until recently, as more EEG headsets become easier to use and available at a lower cost. However, the ability to reliably infer information from the brain using EEG such as intent is limited by poor spatial resolution and susceptibility to noise.

In the last decade, deep learning techniques have made significant advances in machine vision, natural language processing, and speech processing. This success is attributed to the ability of deep neural networks (DNNs) to learn complex and robust distributed representations of inputs \cite{DBLP:journals/corr/abs-1206-5538} when given large numbers of training examples. Because EEG data can be more easily acquired due to the availability of consumer-grade EEG headsets, DNNs present an opportunity for improving performance in BCI. Thus, we developed a deep learning model to classify a constrained set of EEG data.

In section \ref{section:litreview}, we present a detailed discussion of EEG and deep learning. In section \ref{section:data}, we discuss the characteristics of our EEG dataset, features used in EEG signal processing, and the task at hand. In section \ref{section:techniques}, we describe the deep learning and ensemble approaches used in our exploration of the problem. In section \ref{section:experiment}, we describe our experimental setup and evaluation metrics for the task. In section \ref{section:results}, we describe the results we obtained.


\section{Prior work} \label{section:litreview}

%TODO: so like our thing has never actually been done before. actually, this guy did this one thing but it really sucks. so we were like obviously we could do better with technology x.
%-EEG
% -what is EEG
% -EEG patterns (psych stuff/neuro stuff)
% -Dataset (brief)
%-Deep learning
% -what is deep learning
% -deep learning + EEG

\section{Data} \label{section:data}
%TODO: we have big data, oooh. okay maybe not that big but it still takes forever to load 
%-Dataset (details)
% -processing
%-Features
% -raw
% -dtft
%  -windows
% -wavelets
% -other

\section{Techniques} \label{section:techniques}
\subsection{Deep nets}
Deep nets, neural nets composed of many layers, are known to be able to represent highly complex decision boundaries when used with non-linear activation functions.

In our models, we use the rectified linear unit (ReLU) \cite{ICML:2010NairH10}
\[ \mathrm{ReLU}(x) = \max(0, x) \]
as the activation function for the hidden layers. This non-linear function comes with the advantage that it and its derivatives are quick to compute. However, it is not differentiable at zero; this introduces no effects in practice.

One may use a ``leaky'' ReLU
\[ \mathrm{ReLU}(x; k) = \max(kx, x) \]
where the negative arm of the function has a slightly positive slope $0 < k \ll 1$, ensuring non-zero derivatives.

Parameterized rectified linear units (PReLUs) \cite{DBLP:journals/corr/HeZR015} are leaky ReLUs whose slopes are learnable parameters.

Other standard candidates are the sigmoid function
\[ \sigma(x) = \frac{1}{1+e^{-x}} \]
and the hyperbolic tangent function
\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]
as they are differentiable everywhere.

% TODO: describe the layers/architecture more in detail

\subsection{Autoencoders}
Autoencoders are a class of neural network architectures employed for use in representation learning. Autoencoders are fully connected neural networks containing at least one hidden layer, an encoder, and a decoder. The transformation described by the weights going from the input units to the middle hidden layer is the encoder, and the transformation described by the weights going from the middle hidden layer to the output units is the decoder. Typically, the decoder weights are shared as the transpose of the encoder weights with a separate set of bias units.

Autoencoders can be trained in an unsupervised manner by learning the identity function on a set of data. An autoencoder with multiple hidden layers can be represented as a composition of multiple autoencoders, each with one hidden layer. These stacked autoencoders can be trained in a layerwise fashion, i.e. by greedily training the outermost autoencoder, then combining with the next inner autoencoder, initializing with the pretrained weights. So, we consider the behavior of autoencoders with one hidden layer: depending on the dimension of the hidden layers compared to the dimension of the input and output layers, autoencoders fall into two realms of behaviors.

\subsubsection{Hidden layer smaller than input layer}
If the hidden layer has a smaller dimension than the input layer, then in order to accurately approximate the identity function, the autoencoder learns to compress the larger input space into the smaller vector space represented by the hidden layer activations and retain salient features. If linear activation units $f(x) = x$ are used, then autoencoding is equivalent to principal component analysis (PCA). The trained encoder weights can be used to initialize a subset of a larger neural net classifier to expose the same learned features.

\subsubsection{Hidden layer larger than input layer}
If the hidden layer has a larger dimension than the input layer, straightforward training does not learn a meaningful representation because the identity encoding is a trivial optimum for training and will overfit. To force a useful representation, we introduce noise in the input and try to get the network to learn the original output. This causes the network to produce distributed representations of the input data that are useful in reconstructing the data. By adding Gaussian distributed noise to the input vectors during preprocessing, we obtain a denoising autoencoder that becomes resistant to noise. Another method of adding noise is dropout: during training, a fraction $p$ of the input vector dimensions (usually $p = 0.5$) is randomly set to $0$. During testing, the entire input vector is used and the layer weights are multiplied by $\frac{1}{p}$ to restore scaling of activations.

\subsection{Training and batch normalization}
Training is typically done on batches of inputs at once to take advantage of data parallelism techniques to speed up training.
Batch normalization \cite{DBLP:journals/corr/IoffeS15} is a method for training deep neural networks that incorporates normalization a part of the network, allowing for better performance and training speed. In each layer, with each batch that is propagated through the network, the mean and variance is taken within the batch to renormalize each example. Two learnable coefficients $\alpha$ and $\beta$ are trained to rescale the normalized values.
At test time, a global mean and variance is taken over samples drawn from the training set to be used to scale test examples.

\subsection{Ensemble learning}
% TODO
%-Ensemble
% -random forest
% -classifier fusion

\section{Experiment} \label{section:experiment}
%TODO: we're not making up numbers here yo.
%-Cross-validation
%-Parameter search
%-Evaluation
% -baseline classifiers
% -auc, acc, best-cutoff
%-Alternative evaluations
%-Ranking channels for importance

\section{Results} \label{section:results}
%TODO: yeah so um our thing sucks like the other guy's stuff. but it sucks less. so yaya
%-the results

\section{Conclusion} \label{section:conclusion}
%TODO: look at what we did. we think we can do better next time. but yeah we'll need money for ramen so gib pls
%-refer back to the prev work
%-future work

\section*{Acknowledgments}
%TODO: thanks mom


\printbibliography

\end{document}
