\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09
\usepackage[backend=biber]{biblatex}
\bibliography{paper}

\title{Deep learning on psychological arousal classification}


\author{
Yueran Yuan, Jeffrey Zhang, David Rayson, Kai-min Chang\\
Language Technologies Institute\\
Carnegie Mellon University\\
Pittsburgh, PA 15213\\
\texttt{\{yueranyuan, jzhang94, davidr92, kaimin.chang\}@gmail.com}
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version, comment for submission version!

\begin{document}

\maketitle

\begin{abstract}
We present a task for psychological arousal state classification from EEG data and a model for classifying EEG data using deep learning and ensemble techniques. The task is classification of 10-second recordings taken from Emotiv EPOC headsets into the activities performed during each recording. We find that our model outperforms baseline accuracy values established by random forests and SVMs.
\end{abstract}

\section{Introduction} \label{section:introduction}
One of the ongoing challenges of brain-computer interface (BCI) research is the creation of a non-invasive device that can be directly and accurately controlled with one's mental processes. The most studied approach is electroencephalography (EEG), which is a technique that attaches several electrodes to the scalp to measure the electric potential changes caused by neural oscillations in the brain. EEG devices have been difficult to access for non-researchers until recently, as more EEG headsets become easier to use and available at a lower cost. However, the ability to reliably infer information from the brain using EEG such as intent is limited by poor spatial resolution and susceptibility to noise.

In the last decade, deep learning techniques have made significant advances in machine vision, natural language processing, and speech processing. This success is attributed to the ability of deep neural networks (DNNs) to learn complex and robust distributed representations of inputs \cite{DBLP:journals/corr/abs-1206-5538} when given large numbers of training examples. Because EEG data can be more easily acquired due to the availability of consumer-grade EEG headsets, DNNs present an opportunity for improving performance in BCI. Thus, we developed a deep learning model to classify a constrained set of EEG data.

In section \ref{section:litreview}, we present a detailed discussion of EEG and deep learning. In section \ref{section:data}, we discuss the characteristics of our EEG dataset, features used in EEG signal processing, and the task at hand. In section \ref{section:techniques}, we describe the deep learning and ensemble approaches used in our exploration of the problem. In section \ref{section:experiment}, we describe our experimental setup and evaluation metrics for the task. In section \ref{section:results}, we describe the results we obtained.


\section{Prior work} \label{section:litreview}
\subsection{EEG}
The EEG (electroencephalogram) signal is a voltage signal that can be measured on the surface of the scalp, arising from large areas of coordinated neural activity. Rhythmic fluctuations (oscillation patterns) in the EEG signal occur within several particular frequency bands, and the relative level of activity within each frequency band has been associated with brain/emotional states such as focused attentional processing, engagement, and frustration (Marosi et al., 2002; Marosi et al., 2001;
Mundy-Castle, 1951; Berka et al., 2007) and cognitive and memory performance (Gruber et al., 2004; Fernandez et al., 1998; Gevins et al., 1997; Jensen et al., 2007; Klimesch, 1999). The discriminating power of EEG has been demonstrated in a number of studies, achieving 86\% accuracy on 14-way task discrimination, 95\% accuracy on discriminating high/low memory load (Gevins et al., 1998). 
\subsection{IAPS}
\subsection{Deep Learning}
Contemporary deep learning was introduced relatively recently (Hinton, 2006). Prior to the advent of deep learning, classification algorithms by in large operated on some pre-determined preprocessing steps which transforms the raw input into features. These features were then used by a learning algorithm to train a classifier. Deep algorithms, by contrast, combine data processing and classification in the same learning pipeline. This allows the system to learn how to process raw data from
patterns in the data rather than relying on hand-engineered pre-processing steps. This is why deep learning has alternatively been called feature learning or representation learning.
Deep learning excels when it is trained on large repositories of data. So the relatively recent availability of large datasets for speech, computer vision, and text processing has allowed deep learning researchers to train models matching or beating state-of-the-art systems (Boulanger-Lewandowski et al., 2012; Krizhevsky et al., 2012; Miklov et al., 2011).  Deep learning is particularly good at extracting structure and information from raw data with little to no knowledge about the
nature of the data or the nature of the intended classification task by performing unsupervised pre-training (Mesnil et al., 2012). Researchers hypothesize that this is because deep learning algorithms are able to disentangle the many sources of variation (Bengio, 2009).  This is supported by empirical analysis of the learnt features which demonstrate that features that best distinguish task domains are largely separate from features that best distinguish classification conditions (Glorot et
al., 2011), a separation that suggests features are selectively sensitive to only one type of variation.  The usefulness of this disentanglement is evident in the strong performance of deep learning algorithms in transfer learning competitions (Mesnil et al .,2012; Goodfellow et al., 2011).
Disentangling sources of variation is a particularly useful trait for use in EEG. As mentioned earlier, sources of signal in EEG are not well-understood and feature engineering is very difficult. By providing deep learning algorithms with a sufficiently large EEG dataset, the algorithms may be able to disentangle the many sources of noise and signal to recognize the specific patterns associated with mental states of interest and discarding patterns associated with environmental noise.
Further, deep algorithms excel at transferring between different task domains which may be helpful in creating classifiers that can be used on unseen subjects despite non-trivial differences in subject psychology and physiology.
\subsection{Deep Learning and EEG}
Deep learning has also had some very recent success in non-EEG biometrics and laboratory EEG. Mirowski et al. (2008) used a convolutional neural network on intracranial EEG signal, predicting seizures with zero-false-alarm rate on 20 out of 21 patients, beating SVM and logistic regression. Cecotti and Gr√§ser (2010) achieved state-of-the-art results on detecting the P300 event-related-potential with laboratory grade EEG also using convolutional networks. And more recently, Martinez et al.
(2013) used deep learning techniques to produce models of affect from non-EEG biometrics (Skin Conductance and Blood Volume Pulse) which perform significantly better than ad-hoc feature models.
Uniquely, we are using Deep Learning techniques on non-medical-grade EEG devices to track emotion.

\section{Data} \label{section:data}
Our experiment uses data from Siegle et al. (forthcoming). To our knowledge, this is the largest dataset of EEG collected in controlled laboratory conditions (TODO: get x and n) with n unique subjects from x different sites. The dataset contains various stimuli types and our experiments focus on the IAPS stimuli (TODO: cite lang et al 2008). IAPS is a set of images designed to illicit emotional responses along two emotional axes - high, low arousal and positive, negative valence. The stimuli
fall into 4 categories - low arousal positive valence, low arousal negative valence, high arousal positive valence, and high arousal negative valence. In this dataset, each subject viewed 10 images from each category with a 2 (TODO: check this) second fixation period between each stimulus category. One limitation of the experiment was that the images were shown to the subject in the same sequence.

The dataset was collected with Emotiv EPOC EEG devices. The EEG data has 14 channels at 128Hz.  The data was processed with %TODO: asdfasdfasdf how did siegle process the data??

We post-processed the resulting 128Hz signal with DFT and wavelets. We used 5 channel DFT threshholding at 0.1Hz, 4Hz, 8Hz, 12Hz, 30Hz, and 64Hz. We used Debauchey wavelets, trying a variety of wavelet decomposition depths. We also derived a number of features based on the feature set used in recent work of (citation needed) which includes eigen correlation, statistical moments, and Hjorth parameters.
%TODO: we have big data, oooh. okay maybe not that big but it still takes forever to load 
%-Dataset (details)
% -processing
%-Features
% -raw
% -dtft
%  -windows
% -wavelets
% -other

\section{Techniques} \label{section:techniques}
\subsection{Deep nets}
Deep nets, neural nets composed of many layers, are known to be able to represent highly complex decision boundaries when used with non-linear activation functions.

In our models, we use the rectified linear unit (ReLU) \cite{ICML:2010NairH10}
\[ \mathrm{ReLU}(x) = \max(0, x) \]
as the activation function for the hidden layers. This non-linear function comes with the advantage that it and its derivatives are quick to compute. However, it is not differentiable at zero; this introduces no effects in practice.

One may use a ``leaky'' ReLU
\[ \mathrm{ReLU}(x; k) = \max(kx, x) \]
where the negative arm of the function has a slightly positive slope $0 < k \ll 1$, ensuring non-zero derivatives.

Parameterized rectified linear units (PReLUs) \cite{DBLP:journals/corr/HeZR015} are leaky ReLUs whose slopes are learnable parameters.

Other standard candidates are the sigmoid function
\[ \sigma(x) = \frac{1}{1+e^{-x}} \]
and the hyperbolic tangent function
\[ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} \]
as they are differentiable everywhere.

% TODO: describe the layers/architecture more in detail

\subsection{Convolutional Neural Networks}
Convolutional neural networks (CNNs) \cite{DBLP:conf/nips/KrizhevskySH12} are neural networks that are not fully connected: instead, each neuron has a limited field of inputs, which overlaps with the fields of adjacent neurons on its layer.  This technique has found success in image processing, as it mimics the functioning of the human brain's visual cortex.  It has also been applied to EEG analysis \cite{DBLP:journals/pami/CecottiG11}, which we attempt in two different ways.  In the first, time-based convolution, we represent our input sample as a series of time points, each of which consists of a vector of features over several channels; the convolutional neurons' fields consist of overlapping slices of time.  In the second, channel-based convolution, the sample is represented as a series of channels, each of which consists of a vector of data points from that channel over time; the convolutional neurons' fields are sets of channels.  We also use a max-pooling downsampling layer above the convolutional layer; this downsamples the results of the convolution by keeping only the maximum value out of each of a set of non-overlapping regions of the convolutional layer.  Finally, we apply a logistic regression classifier to the results of the downsampling to obtain a classification.

\subsection{Autoencoders}
Autoencoders are a class of neural network architectures employed for use in representation learning. Autoencoders are fully connected neural networks containing at least one hidden layer, an encoder, and a decoder. The transformation described by the weights going from the input units to the middle hidden layer is the encoder, and the transformation described by the weights going from the middle hidden layer to the output units is the decoder. Typically, the decoder weights are shared as the transpose of the encoder weights with a separate set of bias units.

Autoencoders can be trained in an unsupervised manner by learning the identity function on a set of data. An autoencoder with multiple hidden layers can be represented as a composition of multiple autoencoders, each with one hidden layer. These stacked autoencoders can be trained in a layerwise fashion, i.e. by greedily training the outermost autoencoder, then combining with the next inner autoencoder, initializing with the pretrained weights. So, we consider the behavior of autoencoders with one hidden layer: depending on the dimension of the hidden layers compared to the dimension of the input and output layers, autoencoders fall into two realms of behaviors.

\subsubsection{Hidden layer smaller than input layer}
If the hidden layer has a smaller dimension than the input layer, then in order to accurately approximate the identity function, the autoencoder learns to compress the larger input space into the smaller vector space represented by the hidden layer activations and retain salient features. If linear activation units $f(x) = x$ are used, then autoencoding is equivalent to principal component analysis (PCA). The trained encoder weights can be used to initialize a subset of a larger neural net classifier to expose the same learned features.

\subsubsection{Hidden layer larger than input layer}
If the hidden layer has a larger dimension than the input layer, straightforward training does not learn a meaningful representation because the identity encoding is a trivial optimum for training and will overfit. To force a useful representation, we introduce noise in the input and try to get the network to learn the original output. This causes the network to produce distributed representations of the input data that are useful in reconstructing the data. By adding Gaussian distributed noise to the input vectors during preprocessing, we obtain a denoising autoencoder that becomes resistant to noise. Another method of adding noise is dropout: during training, a fraction $p$ of the input vector dimensions (usually $p = 0.5$) is randomly set to $0$. During testing, the entire input vector is used and the layer weights are multiplied by $\frac{1}{p}$ to restore scaling of activations.

\subsection{Training and batch normalization}
Training is typically done on batches of inputs at once to take advantage of data parallelism techniques to speed up training.
Batch normalization \cite{DBLP:journals/corr/IoffeS15} is a method for training deep neural networks that incorporates normalization a part of the network, allowing for better performance and training speed. In each layer, with each batch that is propagated through the network, the mean and variance is taken within the batch to renormalize each example. Two learnable coefficients $\alpha$ and $\beta$ are trained to rescale the normalized values.
At test time, a global mean and variance is taken over samples drawn from the training set to be used to scale test examples.

\subsection{Ensemble learning}
%-Ensemble
% -random forest
% -classifier fusion

\section{Experiment} \label{section:experiment}
%TODO: we're not making up numbers here yo.
%-Cross-validation
We randomly construct a training set and a test set, which consists of about $10\%$ of the total data.  These sets each contain the same proportion of samples of each class, from a random set of subjects.
%-Parameter search
%-Evaluation
\subsection{Evaluation}
% -baseline classifiers
\subsubsection{Baseline Classifiers}
In addition to our deep learning-based classifiers, we compared against two traditional classifiers: a support vector machine (with a degree-2 polynomial kernel function) and a random forest (consisting of 50 trees).
% -auc, acc, best-cutoff
\subsubsection{Accuracy}
The output of our classifiers when classifying input $x$ into category $A$ or $B$ is two probabilities $P(A_x)$ and $P(B_x)$ (the predicted probability that $x$ is in category $A$ or $B$ respectively), with $P(A_x)+P(B_x)=1$.  We choose the greater of these probabilities and declare its category to be the predicted category for $x$.  The accuracy of the classifier over a set of input samples is then simply the percentage of samples for which the predicted and actual categories agree.
\subsubsection{Best-Cutoff Accuracy}
Best-cutoff accuracy is a generalization of our accuracy calculation described above.  The only change is in how the predicted category for a sample is determined: a cutoff $c$ is set, with the sample declared to be in category $A$ if $P(A_x) > c$ and category $B$ otherwise.  Each time that the best-cutoff accuracy is computed for a set of predictions, the value of the cutoff is dynamically chosen such that the final accuracy result is maximized.
\subsubsection{AUC}
The area under the receiver operating characteristic (ROC) curve has been proposed \cite{DBLP:conf/ijcai/LingHZ03} as a more effective metric for comparing classifiers than accuracy.  The ROC curve represents the ratio of the true positive rate of a classifier to its false positive rate as its discrimination cutoff is varied.  We arbitrarily select one class here to be the positive class and the other to be the negative class.  We then construct a ranking of all input samples from least-likely to most-likely to be in the positive class (as determined by the probability assigned to that class by the classifier).  Our discrimination cutoff is then the point in this ranking list for which we determine less-likely-positive samples to be negative and more-likely ones to be positive.  We calculate the area under the ROC curve over all possible values of this cutoff, using this result as our metric.
%-Alternative evaluations
%-Ranking channels for importance

\section{Results} \label{section:results}
%TODO: yeah so um our thing sucks like the other guy's stuff. but it sucks less. so yaya
%-the results

\section{Conclusion} \label{section:conclusion}
%TODO: look at what we did. we think we can do better next time. but yeah we'll need money for ramen so gib pls
%-refer back to the prev work
%-future work

\section*{Acknowledgments}
%TODO: thanks mom


\printbibliography

\end{document}
